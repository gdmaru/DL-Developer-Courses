{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANLP - BERT - Sequence Regression Model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "7ckpEQrJAPmx",
        "Yn9i5fDHAPm7",
        "QZAgzFv0APnC"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gdmaru/DL-Developer-Courses/blob/master/ANLP_BERT_Sequence_Regression_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "sM2lzAXNAPmH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Adapted from [this example script](https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/examples/run_classifier.py)"
      ]
    },
    {
      "metadata": {
        "id": "eMTtOgYMAPmJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Local network environment settings\n",
        "import os\n",
        "os.environ[\"http_proxy\"] = \"127.0.0.1:11233\"\n",
        "os.environ[\"https_proxy\"] = \"127.0.0.1:11233\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VydVguN6APmM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Logger settings and Constants: "
      ]
    },
    {
      "metadata": {
        "id": "U0BJEOVBAPmN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
        "                    level = logging.INFO)\n",
        "logger = logging.getLogger(\"regressor\")\n",
        "\n",
        "FP16 = False\n",
        "BATCH_SIZE = 32\n",
        "SEED = 42\n",
        "WARMUP_PROPORTION = 0.1\n",
        "PYTORCH_PRETRAINED_BERT_CACHE = \"/mnt/Intel/bert_tmp\"\n",
        "LOSS_SCALE = 0. # Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\n",
        "MAX_SEQ_LENGTH = 100\n",
        "\n",
        "DATA_PATH = \"douban_ratings.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MkJnQVizSv7L",
        "colab_type": "code",
        "outputId": "f85afc6f-1592-4a08-b054-0a3a59e75381",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "cell_type": "code",
      "source": [
        "# Upload modeling.py from local drive(pytorch_pretrained_bert)\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-78831a26-bd61-4d5f-a532-53e136192e91\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-78831a26-bd61-4d5f-a532-53e136192e91\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving modeling.py to modeling (2).py\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'modeling.py': b'# coding=utf-8\\n# Copyright 2018 The Google AI Language Team Authors and The HugginFace Inc. team.\\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\"\"\"PyTorch BERT model.\"\"\"\\n\\nfrom __future__ import absolute_import\\nfrom __future__ import division\\nfrom __future__ import print_function\\n\\nimport os\\nimport copy\\nimport json\\nimport math\\nimport logging\\nimport tarfile\\nimport tempfile\\nimport shutil\\n\\nimport torch\\nfrom torch import nn\\nfrom torch.nn import CrossEntropyLoss\\n\\nfrom file_utils import cached_path\\n\\nlogger = logging.getLogger(__name__)\\n\\nPRETRAINED_MODEL_ARCHIVE_MAP = {\\n    \\'bert-base-uncased\\': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz\",\\n    \\'bert-large-uncased\\': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased.tar.gz\",\\n    \\'bert-base-cased\\': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased.tar.gz\",\\n    \\'bert-large-cased\\': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased.tar.gz\",\\n    \\'bert-base-multilingual-uncased\\': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased.tar.gz\",\\n    \\'bert-base-multilingual-cased\\': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased.tar.gz\",\\n    \\'bert-base-chinese\\': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz\",\\n}\\nCONFIG_NAME = \\'bert_config.json\\'\\nWEIGHTS_NAME = \\'pytorch_model.bin\\'\\n\\ndef gelu(x):\\n    \"\"\"Implementation of the gelu activation function.\\n        For information: OpenAI GPT\\'s gelu is slightly different (and gives slightly different results):\\n        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\\n    \"\"\"\\n    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\\n\\n\\ndef swish(x):\\n    return x * torch.sigmoid(x)\\n\\n\\nACT2FN = {\"gelu\": gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish}\\n\\n\\nclass BertConfig(object):\\n    \"\"\"Configuration class to store the configuration of a `BertModel`.\\n    \"\"\"\\n    def __init__(self,\\n                 vocab_size_or_config_json_file,\\n                 hidden_size=768,\\n                 num_hidden_layers=12,\\n                 num_attention_heads=12,\\n                 intermediate_size=3072,\\n                 hidden_act=\"gelu\",\\n                 hidden_dropout_prob=0.1,\\n                 attention_probs_dropout_prob=0.1,\\n                 max_position_embeddings=512,\\n                 type_vocab_size=2,\\n                 initializer_range=0.02):\\n        \"\"\"Constructs BertConfig.\\n\\n        Args:\\n            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `BertModel`.\\n            hidden_size: Size of the encoder layers and the pooler layer.\\n            num_hidden_layers: Number of hidden layers in the Transformer encoder.\\n            num_attention_heads: Number of attention heads for each attention layer in\\n                the Transformer encoder.\\n            intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\\n                layer in the Transformer encoder.\\n            hidden_act: The non-linear activation function (function or string) in the\\n                encoder and pooler. If string, \"gelu\", \"relu\" and \"swish\" are supported.\\n            hidden_dropout_prob: The dropout probabilitiy for all fully connected\\n                layers in the embeddings, encoder, and pooler.\\n            attention_probs_dropout_prob: The dropout ratio for the attention\\n                probabilities.\\n            max_position_embeddings: The maximum sequence length that this model might\\n                ever be used with. Typically set this to something large just in case\\n                (e.g., 512 or 1024 or 2048).\\n            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\\n                `BertModel`.\\n            initializer_range: The sttdev of the truncated_normal_initializer for\\n                initializing all weight matrices.\\n        \"\"\"\\n        if isinstance(vocab_size_or_config_json_file, str):\\n            with open(vocab_size_or_config_json_file, \"r\", encoding=\\'utf-8\\') as reader:\\n                json_config = json.loads(reader.read())\\n            for key, value in json_config.items():\\n                self.__dict__[key] = value\\n        elif isinstance(vocab_size_or_config_json_file, int):\\n            self.vocab_size = vocab_size_or_config_json_file\\n            self.hidden_size = hidden_size\\n            self.num_hidden_layers = num_hidden_layers\\n            self.num_attention_heads = num_attention_heads\\n            self.hidden_act = hidden_act\\n            self.intermediate_size = intermediate_size\\n            self.hidden_dropout_prob = hidden_dropout_prob\\n            self.attention_probs_dropout_prob = attention_probs_dropout_prob\\n            self.max_position_embeddings = max_position_embeddings\\n            self.type_vocab_size = type_vocab_size\\n            self.initializer_range = initializer_range\\n        else:\\n            raise ValueError(\"First argument must be either a vocabulary size (int)\"\\n                             \"or the path to a pretrained model config file (str)\")\\n\\n    @classmethod\\n    def from_dict(cls, json_object):\\n        \"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"\\n        config = BertConfig(vocab_size_or_config_json_file=-1)\\n        for key, value in json_object.items():\\n            config.__dict__[key] = value\\n        return config\\n\\n    @classmethod\\n    def from_json_file(cls, json_file):\\n        \"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"\\n        with open(json_file, \"r\", encoding=\\'utf-8\\') as reader:\\n            text = reader.read()\\n        return cls.from_dict(json.loads(text))\\n\\n    def __repr__(self):\\n        return str(self.to_json_string())\\n\\n    def to_dict(self):\\n        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\\n        output = copy.deepcopy(self.__dict__)\\n        return output\\n\\n    def to_json_string(self):\\n        \"\"\"Serializes this instance to a JSON string.\"\"\"\\n        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\\\n\"\\n\\ntry:\\n    from apex.normalization.fused_layer_norm import FusedLayerNorm as BertLayerNorm\\nexcept ImportError:\\n    print(\"Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\")\\n    class BertLayerNorm(nn.Module):\\n        def __init__(self, hidden_size, eps=1e-12):\\n            \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\\n            \"\"\"\\n            super(BertLayerNorm, self).__init__()\\n            self.weight = nn.Parameter(torch.ones(hidden_size))\\n            self.bias = nn.Parameter(torch.zeros(hidden_size))\\n            self.variance_epsilon = eps\\n\\n        def forward(self, x):\\n            u = x.mean(-1, keepdim=True)\\n            s = (x - u).pow(2).mean(-1, keepdim=True)\\n            x = (x - u) / torch.sqrt(s + self.variance_epsilon)\\n            return self.weight * x + self.bias\\n\\nclass BertEmbeddings(nn.Module):\\n    \"\"\"Construct the embeddings from word, position and token_type embeddings.\\n    \"\"\"\\n    def __init__(self, config):\\n        super(BertEmbeddings, self).__init__()\\n        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\\n        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\\n        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\\n\\n        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\\n        # any TensorFlow checkpoint file\\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\\n\\n    def forward(self, input_ids, token_type_ids=None):\\n        seq_length = input_ids.size(1)\\n        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\\n        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\\n        if token_type_ids is None:\\n            token_type_ids = torch.zeros_like(input_ids)\\n\\n        words_embeddings = self.word_embeddings(input_ids)\\n        position_embeddings = self.position_embeddings(position_ids)\\n        token_type_embeddings = self.token_type_embeddings(token_type_ids)\\n\\n        embeddings = words_embeddings + position_embeddings + token_type_embeddings\\n        embeddings = self.LayerNorm(embeddings)\\n        embeddings = self.dropout(embeddings)\\n        return embeddings\\n\\n\\nclass BertSelfAttention(nn.Module):\\n    def __init__(self, config):\\n        super(BertSelfAttention, self).__init__()\\n        if config.hidden_size % config.num_attention_heads != 0:\\n            raise ValueError(\\n                \"The hidden size (%d) is not a multiple of the number of attention \"\\n                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads))\\n        self.num_attention_heads = config.num_attention_heads\\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\\n\\n        self.query = nn.Linear(config.hidden_size, self.all_head_size)\\n        self.key = nn.Linear(config.hidden_size, self.all_head_size)\\n        self.value = nn.Linear(config.hidden_size, self.all_head_size)\\n\\n        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\\n\\n    def transpose_for_scores(self, x):\\n        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\\n        x = x.view(*new_x_shape)\\n        return x.permute(0, 2, 1, 3)\\n\\n    def forward(self, hidden_states, attention_mask):\\n        mixed_query_layer = self.query(hidden_states)\\n        mixed_key_layer = self.key(hidden_states)\\n        mixed_value_layer = self.value(hidden_states)\\n\\n        query_layer = self.transpose_for_scores(mixed_query_layer)\\n        key_layer = self.transpose_for_scores(mixed_key_layer)\\n        value_layer = self.transpose_for_scores(mixed_value_layer)\\n\\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\\n        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\\n        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\\n        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\\n        attention_scores = attention_scores + attention_mask\\n\\n        # Normalize the attention scores to probabilities.\\n        attention_probs = nn.Softmax(dim=-1)(attention_scores)\\n\\n        # This is actually dropping out entire tokens to attend to, which might\\n        # seem a bit unusual, but is taken from the original Transformer paper.\\n        attention_probs = self.dropout(attention_probs)\\n\\n        context_layer = torch.matmul(attention_probs, value_layer)\\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\\n        context_layer = context_layer.view(*new_context_layer_shape)\\n        return context_layer\\n\\n\\nclass BertSelfOutput(nn.Module):\\n    def __init__(self, config):\\n        super(BertSelfOutput, self).__init__()\\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\\n\\n    def forward(self, hidden_states, input_tensor):\\n        hidden_states = self.dense(hidden_states)\\n        hidden_states = self.dropout(hidden_states)\\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\\n        return hidden_states\\n\\n\\nclass BertAttention(nn.Module):\\n    def __init__(self, config):\\n        super(BertAttention, self).__init__()\\n        self.self = BertSelfAttention(config)\\n        self.output = BertSelfOutput(config)\\n\\n    def forward(self, input_tensor, attention_mask):\\n        self_output = self.self(input_tensor, attention_mask)\\n        attention_output = self.output(self_output, input_tensor)\\n        return attention_output\\n\\n\\nclass BertIntermediate(nn.Module):\\n    def __init__(self, config):\\n        super(BertIntermediate, self).__init__()\\n        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\\n        self.intermediate_act_fn = ACT2FN[config.hidden_act] \\\\\\n            if isinstance(config.hidden_act, str) else config.hidden_act\\n\\n    def forward(self, hidden_states):\\n        hidden_states = self.dense(hidden_states)\\n        hidden_states = self.intermediate_act_fn(hidden_states)\\n        return hidden_states\\n\\n\\nclass BertOutput(nn.Module):\\n    def __init__(self, config):\\n        super(BertOutput, self).__init__()\\n        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\\n\\n    def forward(self, hidden_states, input_tensor):\\n        hidden_states = self.dense(hidden_states)\\n        hidden_states = self.dropout(hidden_states)\\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\\n        return hidden_states\\n\\n\\nclass BertLayer(nn.Module):\\n    def __init__(self, config):\\n        super(BertLayer, self).__init__()\\n        self.attention = BertAttention(config)\\n        self.intermediate = BertIntermediate(config)\\n        self.output = BertOutput(config)\\n\\n    def forward(self, hidden_states, attention_mask):\\n        attention_output = self.attention(hidden_states, attention_mask)\\n        intermediate_output = self.intermediate(attention_output)\\n        layer_output = self.output(intermediate_output, attention_output)\\n        return layer_output\\n\\n\\nclass BertEncoder(nn.Module):\\n    def __init__(self, config):\\n        super(BertEncoder, self).__init__()\\n        layer = BertLayer(config)\\n        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(config.num_hidden_layers)])\\n\\n    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True):\\n        all_encoder_layers = []\\n        for layer_module in self.layer:\\n            hidden_states = layer_module(hidden_states, attention_mask)\\n            if output_all_encoded_layers:\\n                all_encoder_layers.append(hidden_states)\\n        if not output_all_encoded_layers:\\n            all_encoder_layers.append(hidden_states)\\n        return all_encoder_layers\\n\\n\\nclass BertPooler(nn.Module):\\n    def __init__(self, config):\\n        super(BertPooler, self).__init__()\\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\\n        self.activation = nn.Tanh()\\n\\n    def forward(self, hidden_states):\\n        # We \"pool\" the model by simply taking the hidden state corresponding\\n        # to the first token.\\n        first_token_tensor = hidden_states[:, 0]\\n        pooled_output = self.dense(first_token_tensor)\\n        pooled_output = self.activation(pooled_output)\\n        return pooled_output\\n\\n\\nclass BertPredictionHeadTransform(nn.Module):\\n    def __init__(self, config):\\n        super(BertPredictionHeadTransform, self).__init__()\\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\\n        self.transform_act_fn = ACT2FN[config.hidden_act] \\\\\\n            if isinstance(config.hidden_act, str) else config.hidden_act\\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\\n\\n    def forward(self, hidden_states):\\n        hidden_states = self.dense(hidden_states)\\n        hidden_states = self.transform_act_fn(hidden_states)\\n        hidden_states = self.LayerNorm(hidden_states)\\n        return hidden_states\\n\\n\\nclass BertLMPredictionHead(nn.Module):\\n    def __init__(self, config, bert_model_embedding_weights):\\n        super(BertLMPredictionHead, self).__init__()\\n        self.transform = BertPredictionHeadTransform(config)\\n\\n        # The output weights are the same as the input embeddings, but there is\\n        # an output-only bias for each token.\\n        self.decoder = nn.Linear(bert_model_embedding_weights.size(1),\\n                                 bert_model_embedding_weights.size(0),\\n                                 bias=False)\\n        self.decoder.weight = bert_model_embedding_weights\\n        self.bias = nn.Parameter(torch.zeros(bert_model_embedding_weights.size(0)))\\n\\n    def forward(self, hidden_states):\\n        hidden_states = self.transform(hidden_states)\\n        hidden_states = self.decoder(hidden_states) + self.bias\\n        return hidden_states\\n\\n\\nclass BertOnlyMLMHead(nn.Module):\\n    def __init__(self, config, bert_model_embedding_weights):\\n        super(BertOnlyMLMHead, self).__init__()\\n        self.predictions = BertLMPredictionHead(config, bert_model_embedding_weights)\\n\\n    def forward(self, sequence_output):\\n        prediction_scores = self.predictions(sequence_output)\\n        return prediction_scores\\n\\n\\nclass BertOnlyNSPHead(nn.Module):\\n    def __init__(self, config):\\n        super(BertOnlyNSPHead, self).__init__()\\n        self.seq_relationship = nn.Linear(config.hidden_size, 2)\\n\\n    def forward(self, pooled_output):\\n        seq_relationship_score = self.seq_relationship(pooled_output)\\n        return seq_relationship_score\\n\\n\\nclass BertPreTrainingHeads(nn.Module):\\n    def __init__(self, config, bert_model_embedding_weights):\\n        super(BertPreTrainingHeads, self).__init__()\\n        self.predictions = BertLMPredictionHead(config, bert_model_embedding_weights)\\n        self.seq_relationship = nn.Linear(config.hidden_size, 2)\\n\\n    def forward(self, sequence_output, pooled_output):\\n        prediction_scores = self.predictions(sequence_output)\\n        seq_relationship_score = self.seq_relationship(pooled_output)\\n        return prediction_scores, seq_relationship_score\\n\\n\\nclass PreTrainedBertModel(nn.Module):\\n    \"\"\" An abstract class to handle weights initialization and\\n        a simple interface for dowloading and loading pretrained models.\\n    \"\"\"\\n    def __init__(self, config, *inputs, **kwargs):\\n        super(PreTrainedBertModel, self).__init__()\\n        if not isinstance(config, BertConfig):\\n            raise ValueError(\\n                \"Parameter config in `{}(config)` should be an instance of class `BertConfig`. \"\\n                \"To create a model from a Google pretrained model use \"\\n                \"`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`\".format(\\n                    self.__class__.__name__, self.__class__.__name__\\n                ))\\n        self.config = config\\n\\n    def init_bert_weights(self, module):\\n        \"\"\" Initialize the weights.\\n        \"\"\"\\n        if isinstance(module, (nn.Linear, nn.Embedding)):\\n            # Slightly different from the TF version which uses truncated_normal for initialization\\n            # cf https://github.com/pytorch/pytorch/pull/5617\\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\\n        elif isinstance(module, BertLayerNorm):\\n            module.bias.data.zero_()\\n            module.weight.data.fill_(1.0)\\n        if isinstance(module, nn.Linear) and module.bias is not None:\\n            module.bias.data.zero_()\\n\\n    @classmethod\\n    def from_pretrained(cls, pretrained_model_name, state_dict=None, cache_dir=None, *inputs, **kwargs):\\n        \"\"\"\\n        Instantiate a PreTrainedBertModel from a pre-trained model file or a pytorch state dict.\\n        Download and cache the pre-trained model file if needed.\\n\\n        Params:\\n            pretrained_model_name: either:\\n                - a str with the name of a pre-trained model to load selected in the list of:\\n                    . `bert-base-uncased`\\n                    . `bert-large-uncased`\\n                    . `bert-base-cased`\\n                    . `bert-large-cased`\\n                    . `bert-base-multilingual-uncased`\\n                    . `bert-base-multilingual-cased`\\n                    . `bert-base-chinese`\\n                - a path or url to a pretrained model archive containing:\\n                    . `bert_config.json` a configuration file for the model\\n                    . `pytorch_model.bin` a PyTorch dump of a BertForPreTraining instance\\n            cache_dir: an optional path to a folder in which the pre-trained models will be cached.\\n            state_dict: an optional state dictionnary (collections.OrderedDict object) to use instead of Google pre-trained models\\n            *inputs, **kwargs: additional input for the specific Bert class\\n                (ex: num_labels for BertForSequenceClassification)\\n        \"\"\"\\n        if pretrained_model_name in PRETRAINED_MODEL_ARCHIVE_MAP:\\n            archive_file = PRETRAINED_MODEL_ARCHIVE_MAP[pretrained_model_name]\\n        else:\\n            archive_file = pretrained_model_name\\n        # redirect to the cache, if necessary\\n        try:\\n            resolved_archive_file = cached_path(archive_file, cache_dir=cache_dir)\\n        except FileNotFoundError:\\n            logger.error(\\n                \"Model name \\'{}\\' was not found in model name list ({}). \"\\n                \"We assumed \\'{}\\' was a path or url but couldn\\'t find any file \"\\n                \"associated to this path or url.\".format(\\n                    pretrained_model_name,\\n                    \\', \\'.join(PRETRAINED_MODEL_ARCHIVE_MAP.keys()),\\n                    archive_file))\\n            return None\\n        if resolved_archive_file == archive_file:\\n            logger.info(\"loading archive file {}\".format(archive_file))\\n        else:\\n            logger.info(\"loading archive file {} from cache at {}\".format(\\n                archive_file, resolved_archive_file))\\n        tempdir = None\\n        if os.path.isdir(resolved_archive_file):\\n            serialization_dir = resolved_archive_file\\n        else:\\n            # Extract archive to temp dir\\n            tempdir = tempfile.mkdtemp()\\n            logger.info(\"extracting archive file {} to temp dir {}\".format(\\n                resolved_archive_file, tempdir))\\n            with tarfile.open(resolved_archive_file, \\'r:gz\\') as archive:\\n                archive.extractall(tempdir)\\n            serialization_dir = tempdir\\n        # Load config\\n        config_file = os.path.join(serialization_dir, CONFIG_NAME)\\n        config = BertConfig.from_json_file(config_file)\\n        logger.info(\"Model config {}\".format(config))\\n        # Instantiate model.\\n        model = cls(config, *inputs, **kwargs)\\n        if state_dict is None:\\n            weights_path = os.path.join(serialization_dir, WEIGHTS_NAME)\\n            state_dict = torch.load(weights_path)\\n\\n        old_keys = []\\n        new_keys = []\\n        for key in state_dict.keys():\\n            new_key = None\\n            if \\'gamma\\' in key:\\n                new_key = key.replace(\\'gamma\\', \\'weight\\')\\n            if \\'beta\\' in key:\\n                new_key = key.replace(\\'beta\\', \\'bias\\')\\n            if new_key:\\n                old_keys.append(key)\\n                new_keys.append(new_key)\\n        for old_key, new_key in zip(old_keys, new_keys):\\n            state_dict[new_key] = state_dict.pop(old_key)\\n\\n        missing_keys = []\\n        unexpected_keys = []\\n        error_msgs = []\\n        # copy state_dict so _load_from_state_dict can modify it\\n        metadata = getattr(state_dict, \\'_metadata\\', None)\\n        state_dict = state_dict.copy()\\n        if metadata is not None:\\n            state_dict._metadata = metadata\\n\\n        def load(module, prefix=\\'\\'):\\n            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\\n            module._load_from_state_dict(\\n                state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\\n            for name, child in module._modules.items():\\n                if child is not None:\\n                    load(child, prefix + name + \\'.\\')\\n        load(model, prefix=\\'\\' if hasattr(model, \\'bert\\') else \\'bert.\\')\\n        if len(missing_keys) > 0:\\n            logger.info(\"Weights of {} not initialized from pretrained model: {}\".format(\\n                model.__class__.__name__, missing_keys))\\n        if len(unexpected_keys) > 0:\\n            logger.info(\"Weights from pretrained model not used in {}: {}\".format(\\n                model.__class__.__name__, unexpected_keys))\\n        if tempdir:\\n            # Clean up temp dir\\n            shutil.rmtree(tempdir)\\n        return model\\n\\n\\nclass BertModel(PreTrainedBertModel):\\n    \"\"\"BERT model (\"Bidirectional Embedding Representations from a Transformer\").\\n\\n    Params:\\n        config: a BertConfig class instance with the configuration to build a new model\\n\\n    Inputs:\\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\\n            a `sentence B` token (see BERT paper for more details).\\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\\n            selected in [0, 1]. It\\'s a mask to be used if the input sequence length is smaller than the max\\n            input sequence length in the current batch. It\\'s the mask that we typically use for attention when\\n            a batch has varying length sentences.\\n        `output_all_encoded_layers`: boolean which controls the content of the `encoded_layers` output as described below. Default: `True`.\\n\\n    Outputs: Tuple of (encoded_layers, pooled_output)\\n        `encoded_layers`: controled by `output_all_encoded_layers` argument:\\n            - `output_all_encoded_layers=True`: outputs a list of the full sequences of encoded-hidden-states at the end\\n                of each attention block (i.e. 12 full sequences for BERT-base, 24 for BERT-large), each\\n                encoded-hidden-state is a torch.FloatTensor of size [batch_size, sequence_length, hidden_size],\\n            - `output_all_encoded_layers=False`: outputs only the full sequence of hidden-states corresponding\\n                to the last attention block of shape [batch_size, sequence_length, hidden_size],\\n        `pooled_output`: a torch.FloatTensor of size [batch_size, hidden_size] which is the output of a\\n            classifier pretrained on top of the hidden state associated to the first character of the\\n            input (`CLF`) to train on the Next-Sentence task (see BERT\\'s paper).\\n\\n    Example usage:\\n    ```python\\n    # Already been converted into WordPiece token ids\\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\\n\\n    config = modeling.BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\\n\\n    model = modeling.BertModel(config=config)\\n    all_encoder_layers, pooled_output = model(input_ids, token_type_ids, input_mask)\\n    ```\\n    \"\"\"\\n    def __init__(self, config):\\n        super(BertModel, self).__init__(config)\\n        self.embeddings = BertEmbeddings(config)\\n        self.encoder = BertEncoder(config)\\n        self.pooler = BertPooler(config)\\n        self.apply(self.init_bert_weights)\\n\\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, output_all_encoded_layers=True):\\n        if attention_mask is None:\\n            attention_mask = torch.ones_like(input_ids)\\n        if token_type_ids is None:\\n            token_type_ids = torch.zeros_like(input_ids)\\n\\n        # We create a 3D attention mask from a 2D tensor mask.\\n        # Sizes are [batch_size, 1, 1, to_seq_length]\\n        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\\n        # this attention mask is more simple than the triangular masking of causal attention\\n        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\\n        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\\n\\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\\n        # masked positions, this operation will create a tensor which is 0.0 for\\n        # positions we want to attend and -10000.0 for masked positions.\\n        # Since we are adding it to the raw scores before the softmax, this is\\n        # effectively the same as removing these entirely.\\n        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\\n\\n        embedding_output = self.embeddings(input_ids, token_type_ids)\\n        encoded_layers = self.encoder(embedding_output,\\n                                      extended_attention_mask,\\n                                      output_all_encoded_layers=output_all_encoded_layers)\\n        sequence_output = encoded_layers[-1]\\n        pooled_output = self.pooler(sequence_output)\\n        if not output_all_encoded_layers:\\n            encoded_layers = encoded_layers[-1]\\n        return encoded_layers, pooled_output\\n\\n\\nclass BertForPreTraining(PreTrainedBertModel):\\n    \"\"\"BERT model with pre-training heads.\\n    This module comprises the BERT model followed by the two pre-training heads:\\n        - the masked language modeling head, and\\n        - the next sentence classification head.\\n\\n    Params:\\n        config: a BertConfig class instance with the configuration to build a new model.\\n\\n    Inputs:\\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\\n            a `sentence B` token (see BERT paper for more details).\\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\\n            selected in [0, 1]. It\\'s a mask to be used if the input sequence length is smaller than the max\\n            input sequence length in the current batch. It\\'s the mask that we typically use for attention when\\n            a batch has varying length sentences.\\n        `masked_lm_labels`: masked language modeling labels: torch.LongTensor of shape [batch_size, sequence_length]\\n            with indices selected in [-1, 0, ..., vocab_size]. All labels set to -1 are ignored (masked), the loss\\n            is only computed for the labels set in [0, ..., vocab_size]\\n        `next_sentence_label`: next sentence classification loss: torch.LongTensor of shape [batch_size]\\n            with indices selected in [0, 1].\\n            0 => next sentence is the continuation, 1 => next sentence is a random sentence.\\n\\n    Outputs:\\n        if `masked_lm_labels` and `next_sentence_label` are not `None`:\\n            Outputs the total_loss which is the sum of the masked language modeling loss and the next\\n            sentence classification loss.\\n        if `masked_lm_labels` or `next_sentence_label` is `None`:\\n            Outputs a tuple comprising\\n            - the masked language modeling logits of shape [batch_size, sequence_length, vocab_size], and\\n            - the next sentence classification logits of shape [batch_size, 2].\\n\\n    Example usage:\\n    ```python\\n    # Already been converted into WordPiece token ids\\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\\n\\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\\n\\n    model = BertForPreTraining(config)\\n    masked_lm_logits_scores, seq_relationship_logits = model(input_ids, token_type_ids, input_mask)\\n    ```\\n    \"\"\"\\n    def __init__(self, config):\\n        super(BertForPreTraining, self).__init__(config)\\n        self.bert = BertModel(config)\\n        self.cls = BertPreTrainingHeads(config, self.bert.embeddings.word_embeddings.weight)\\n        self.apply(self.init_bert_weights)\\n\\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None, next_sentence_label=None):\\n        sequence_output, pooled_output = self.bert(input_ids, token_type_ids, attention_mask,\\n                                                   output_all_encoded_layers=False)\\n        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\\n\\n        if masked_lm_labels is not None and next_sentence_label is not None:\\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\\n            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\\n            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\\n            total_loss = masked_lm_loss + next_sentence_loss\\n            return total_loss\\n        else:\\n            return prediction_scores, seq_relationship_score\\n\\n\\nclass BertForMaskedLM(PreTrainedBertModel):\\n    \"\"\"BERT model with the masked language modeling head.\\n    This module comprises the BERT model followed by the masked language modeling head.\\n\\n    Params:\\n        config: a BertConfig class instance with the configuration to build a new model.\\n\\n    Inputs:\\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\\n            a `sentence B` token (see BERT paper for more details).\\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\\n            selected in [0, 1]. It\\'s a mask to be used if the input sequence length is smaller than the max\\n            input sequence length in the current batch. It\\'s the mask that we typically use for attention when\\n            a batch has varying length sentences.\\n        `masked_lm_labels`: masked language modeling labels: torch.LongTensor of shape [batch_size, sequence_length]\\n            with indices selected in [-1, 0, ..., vocab_size]. All labels set to -1 are ignored (masked), the loss\\n            is only computed for the labels set in [0, ..., vocab_size]\\n\\n    Outputs:\\n        if `masked_lm_labels` is  not `None`:\\n            Outputs the masked language modeling loss.\\n        if `masked_lm_labels` is `None`:\\n            Outputs the masked language modeling logits of shape [batch_size, sequence_length, vocab_size].\\n\\n    Example usage:\\n    ```python\\n    # Already been converted into WordPiece token ids\\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\\n\\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\\n\\n    model = BertForMaskedLM(config)\\n    masked_lm_logits_scores = model(input_ids, token_type_ids, input_mask)\\n    ```\\n    \"\"\"\\n    def __init__(self, config):\\n        super(BertForMaskedLM, self).__init__(config)\\n        self.bert = BertModel(config)\\n        self.cls = BertOnlyMLMHead(config, self.bert.embeddings.word_embeddings.weight)\\n        self.apply(self.init_bert_weights)\\n\\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None):\\n        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask,\\n                                       output_all_encoded_layers=False)\\n        prediction_scores = self.cls(sequence_output)\\n\\n        if masked_lm_labels is not None:\\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\\n            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\\n            return masked_lm_loss\\n        else:\\n            return prediction_scores\\n\\n\\nclass BertForNextSentencePrediction(PreTrainedBertModel):\\n    \"\"\"BERT model with next sentence prediction head.\\n    This module comprises the BERT model followed by the next sentence classification head.\\n\\n    Params:\\n        config: a BertConfig class instance with the configuration to build a new model.\\n\\n    Inputs:\\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\\n            a `sentence B` token (see BERT paper for more details).\\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\\n            selected in [0, 1]. It\\'s a mask to be used if the input sequence length is smaller than the max\\n            input sequence length in the current batch. It\\'s the mask that we typically use for attention when\\n            a batch has varying length sentences.\\n        `next_sentence_label`: next sentence classification loss: torch.LongTensor of shape [batch_size]\\n            with indices selected in [0, 1].\\n            0 => next sentence is the continuation, 1 => next sentence is a random sentence.\\n\\n    Outputs:\\n        if `next_sentence_label` is not `None`:\\n            Outputs the total_loss which is the sum of the masked language modeling loss and the next\\n            sentence classification loss.\\n        if `next_sentence_label` is `None`:\\n            Outputs the next sentence classification logits of shape [batch_size, 2].\\n\\n    Example usage:\\n    ```python\\n    # Already been converted into WordPiece token ids\\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\\n\\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\\n\\n    model = BertForNextSentencePrediction(config)\\n    seq_relationship_logits = model(input_ids, token_type_ids, input_mask)\\n    ```\\n    \"\"\"\\n    def __init__(self, config):\\n        super(BertForNextSentencePrediction, self).__init__(config)\\n        self.bert = BertModel(config)\\n        self.cls = BertOnlyNSPHead(config)\\n        self.apply(self.init_bert_weights)\\n\\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, next_sentence_label=None):\\n        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask,\\n                                     output_all_encoded_layers=False)\\n        seq_relationship_score = self.cls( pooled_output)\\n\\n        if next_sentence_label is not None:\\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\\n            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\\n            return next_sentence_loss\\n        else:\\n            return seq_relationship_score\\n\\n\\nclass BertForSequenceClassification(PreTrainedBertModel):\\n    \"\"\"BERT model for classification.\\n    This module is composed of the BERT model with a linear layer on top of\\n    the pooled output.\\n\\n    Params:\\n        `config`: a BertConfig class instance with the configuration to build a new model.\\n        `num_labels`: the number of classes for the classifier. Default = 2.\\n\\n    Inputs:\\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\\n            a `sentence B` token (see BERT paper for more details).\\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\\n            selected in [0, 1]. It\\'s a mask to be used if the input sequence length is smaller than the max\\n            input sequence length in the current batch. It\\'s the mask that we typically use for attention when\\n            a batch has varying length sentences.\\n        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]\\n            with indices selected in [0, ..., num_labels].\\n\\n    Outputs:\\n        if `labels` is not `None`:\\n            Outputs the CrossEntropy classification loss of the output with the labels.\\n        if `labels` is `None`:\\n            Outputs the classification logits of shape [batch_size, num_labels].\\n\\n    Example usage:\\n    ```python\\n    # Already been converted into WordPiece token ids\\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\\n\\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\\n\\n    num_labels = 2\\n\\n    model = BertForSequenceClassification(config, num_labels)\\n    logits = model(input_ids, token_type_ids, input_mask)\\n    ```\\n    \"\"\"\\n    def __init__(self, config, num_labels=2):\\n        super(BertForSequenceClassification, self).__init__(config)\\n        self.num_labels = num_labels\\n        self.bert = BertModel(config)\\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\\n        self.classifier = nn.Linear(config.hidden_size, num_labels)\\n        self.apply(self.init_bert_weights)\\n\\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\\n        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\\n        pooled_output = self.dropout(pooled_output)\\n        logits = self.classifier(pooled_output)\\n\\n        if labels is not None:\\n            loss_fct = CrossEntropyLoss()\\n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\\n            return loss\\n        else:\\n            return logits\\n\\n\\nclass BertForMultipleChoice(PreTrainedBertModel):\\n    \"\"\"BERT model for multiple choice tasks.\\n    This module is composed of the BERT model with a linear layer on top of\\n    the pooled output.\\n\\n    Params:\\n        `config`: a BertConfig class instance with the configuration to build a new model.\\n        `num_choices`: the number of classes for the classifier. Default = 2.\\n\\n    Inputs:\\n        `input_ids`: a torch.LongTensor of shape [batch_size, num_choices, sequence_length]\\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, num_choices, sequence_length]\\n            with the token types indices selected in [0, 1]. Type 0 corresponds to a `sentence A`\\n            and type 1 corresponds to a `sentence B` token (see BERT paper for more details).\\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, num_choices, sequence_length] with indices\\n            selected in [0, 1]. It\\'s a mask to be used if the input sequence length is smaller than the max\\n            input sequence length in the current batch. It\\'s the mask that we typically use for attention when\\n            a batch has varying length sentences.\\n        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]\\n            with indices selected in [0, ..., num_choices].\\n\\n    Outputs:\\n        if `labels` is not `None`:\\n            Outputs the CrossEntropy classification loss of the output with the labels.\\n        if `labels` is `None`:\\n            Outputs the classification logits of shape [batch_size, num_labels].\\n\\n    Example usage:\\n    ```python\\n    # Already been converted into WordPiece token ids\\n    input_ids = torch.LongTensor([[[31, 51, 99], [15, 5, 0]], [[12, 16, 42], [14, 28, 57]]])\\n    input_mask = torch.LongTensor([[[1, 1, 1], [1, 1, 0]],[[1,1,0], [1, 0, 0]]])\\n    token_type_ids = torch.LongTensor([[[0, 0, 1], [0, 1, 0]],[[0, 1, 1], [0, 0, 1]]])\\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\\n\\n    num_choices = 2\\n\\n    model = BertForMultipleChoice(config, num_choices)\\n    logits = model(input_ids, token_type_ids, input_mask)\\n    ```\\n    \"\"\"\\n    def __init__(self, config, num_choices=2):\\n        super(BertForMultipleChoice, self).__init__(config)\\n        self.num_choices = num_choices\\n        self.bert = BertModel(config)\\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\\n        self.classifier = nn.Linear(config.hidden_size, 1)\\n        self.apply(self.init_bert_weights)\\n\\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\\n        flat_input_ids = input_ids.view(-1, input_ids.size(-1))\\n        flat_token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1))\\n        flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1))\\n        _, pooled_output = self.bert(flat_input_ids, flat_token_type_ids, flat_attention_mask, output_all_encoded_layers=False)\\n        pooled_output = self.dropout(pooled_output)\\n        logits = self.classifier(pooled_output)\\n        reshaped_logits = logits.view(-1, self.num_choices)\\n\\n        if labels is not None:\\n            loss_fct = CrossEntropyLoss()\\n            loss = loss_fct(reshaped_logits, labels)\\n            return loss\\n        else:\\n            return reshaped_logits\\n\\n\\nclass BertForTokenClassification(PreTrainedBertModel):\\n    \"\"\"BERT model for token-level classification.\\n    This module is composed of the BERT model with a linear layer on top of\\n    the full hidden state of the last layer.\\n\\n    Params:\\n        `config`: a BertConfig class instance with the configuration to build a new model.\\n        `num_labels`: the number of classes for the classifier. Default = 2.\\n\\n    Inputs:\\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\\n            a `sentence B` token (see BERT paper for more details).\\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\\n            selected in [0, 1]. It\\'s a mask to be used if the input sequence length is smaller than the max\\n            input sequence length in the current batch. It\\'s the mask that we typically use for attention when\\n            a batch has varying length sentences.\\n        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size, sequence_length]\\n            with indices selected in [0, ..., num_labels].\\n\\n    Outputs:\\n        if `labels` is not `None`:\\n            Outputs the CrossEntropy classification loss of the output with the labels.\\n        if `labels` is `None`:\\n            Outputs the classification logits of shape [batch_size, sequence_length, num_labels].\\n\\n    Example usage:\\n    ```python\\n    # Already been converted into WordPiece token ids\\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\\n\\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\\n\\n    num_labels = 2\\n\\n    model = BertForTokenClassification(config, num_labels)\\n    logits = model(input_ids, token_type_ids, input_mask)\\n    ```\\n    \"\"\"\\n    def __init__(self, config, num_labels=2):\\n        super(BertForTokenClassification, self).__init__(config)\\n        self.num_labels = num_labels\\n        self.bert = BertModel(config)\\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\\n        self.classifier = nn.Linear(config.hidden_size, num_labels)\\n        self.apply(self.init_bert_weights)\\n\\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\\n        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\\n        sequence_output = self.dropout(sequence_output)\\n        logits = self.classifier(sequence_output)\\n\\n        if labels is not None:\\n            loss_fct = CrossEntropyLoss()\\n            # Only keep active parts of the loss\\n            if attention_mask is not None:\\n                active_loss = attention_mask.view(-1) == 1\\n                active_logits = logits.view(-1, self.num_labels)[active_loss]\\n                active_labels = labels.view(-1)[active_loss]\\n                loss = loss_fct(active_logits, active_labels)\\n            else:\\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\\n            return loss\\n        else:\\n            return logits\\n\\n\\nclass BertForQuestionAnswering(PreTrainedBertModel):\\n    \"\"\"BERT model for Question Answering (span extraction).\\n    This module is composed of the BERT model with a linear layer on top of\\n    the sequence output that computes start_logits and end_logits\\n\\n    Params:\\n        `config`: a BertConfig class instance with the configuration to build a new model.\\n\\n    Inputs:\\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\\n            a `sentence B` token (see BERT paper for more details).\\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\\n            selected in [0, 1]. It\\'s a mask to be used if the input sequence length is smaller than the max\\n            input sequence length in the current batch. It\\'s the mask that we typically use for attention when\\n            a batch has varying length sentences.\\n        `start_positions`: position of the first token for the labeled span: torch.LongTensor of shape [batch_size].\\n            Positions are clamped to the length of the sequence and position outside of the sequence are not taken\\n            into account for computing the loss.\\n        `end_positions`: position of the last token for the labeled span: torch.LongTensor of shape [batch_size].\\n            Positions are clamped to the length of the sequence and position outside of the sequence are not taken\\n            into account for computing the loss.\\n\\n    Outputs:\\n        if `start_positions` and `end_positions` are not `None`:\\n            Outputs the total_loss which is the sum of the CrossEntropy loss for the start and end token positions.\\n        if `start_positions` or `end_positions` is `None`:\\n            Outputs a tuple of start_logits, end_logits which are the logits respectively for the start and end\\n            position tokens of shape [batch_size, sequence_length].\\n\\n    Example usage:\\n    ```python\\n    # Already been converted into WordPiece token ids\\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\\n\\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\\n\\n    model = BertForQuestionAnswering(config)\\n    start_logits, end_logits = model(input_ids, token_type_ids, input_mask)\\n    ```\\n    \"\"\"\\n    def __init__(self, config):\\n        super(BertForQuestionAnswering, self).__init__(config)\\n        self.bert = BertModel(config)\\n        # TODO check with Google if it\\'s normal there is no dropout on the token classifier of SQuAD in the TF version\\n        # self.dropout = nn.Dropout(config.hidden_dropout_prob)\\n        self.qa_outputs = nn.Linear(config.hidden_size, 2)\\n        self.apply(self.init_bert_weights)\\n\\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, start_positions=None, end_positions=None):\\n        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\\n        logits = self.qa_outputs(sequence_output)\\n        start_logits, end_logits = logits.split(1, dim=-1)\\n        start_logits = start_logits.squeeze(-1)\\n        end_logits = end_logits.squeeze(-1)\\n\\n        if start_positions is not None and end_positions is not None:\\n            # If we are on multi-GPU, split add a dimension\\n            if len(start_positions.size()) > 1:\\n                start_positions = start_positions.squeeze(-1)\\n            if len(end_positions.size()) > 1:\\n                end_positions = end_positions.squeeze(-1)\\n            # sometimes the start/end positions are outside our model inputs, we ignore these terms\\n            ignored_index = start_logits.size(1)\\n            start_positions.clamp_(0, ignored_index)\\n            end_positions.clamp_(0, ignored_index)\\n\\n            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\\n            start_loss = loss_fct(start_logits, start_positions)\\n            end_loss = loss_fct(end_logits, end_positions)\\n            total_loss = (start_loss + end_loss) / 2\\n            return total_loss\\n        else:\\n            return start_logits, end_logits\\n'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "u2gg-tbSSwvg",
        "colab_type": "code",
        "outputId": "72635168-10ba-426c-abd3-6c4574aae959",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "cell_type": "code",
      "source": [
        "# Upload tokenization.py from local drive\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-cd891a27-7159-498e-a45a-1811cb6d4fef\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-cd891a27-7159-498e-a45a-1811cb6d4fef\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving tokenization.py to tokenization (2).py\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'tokenization.py': b'# coding=utf-8\\n# Copyright 2018 The Google AI Language Team Authors and The HugginFace Inc. team.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\"\"\"Tokenization classes.\"\"\"\\n\\nfrom __future__ import absolute_import\\nfrom __future__ import division\\nfrom __future__ import print_function\\n\\nimport collections\\nimport unicodedata\\nimport os\\nimport logging\\n\\nfrom file_utils import cached_path\\n\\nlogger = logging.getLogger(__name__)\\n\\nPRETRAINED_VOCAB_ARCHIVE_MAP = {\\n    \\'bert-base-uncased\\': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\",\\n    \\'bert-large-uncased\\': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt\",\\n    \\'bert-base-cased\\': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt\",\\n    \\'bert-large-cased\\': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-vocab.txt\",\\n    \\'bert-base-multilingual-uncased\\': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt\",\\n    \\'bert-base-multilingual-cased\\': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt\",\\n    \\'bert-base-chinese\\': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt\",\\n}\\nPRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP = {\\n    \\'bert-base-uncased\\': 512,\\n    \\'bert-large-uncased\\': 512,\\n    \\'bert-base-cased\\': 512,\\n    \\'bert-large-cased\\': 512,\\n    \\'bert-base-multilingual-uncased\\': 512,\\n    \\'bert-base-multilingual-cased\\': 512,\\n    \\'bert-base-chinese\\': 512,\\n}\\nVOCAB_NAME = \\'vocab.txt\\'\\n\\n\\ndef load_vocab(vocab_file):\\n    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\\n    vocab = collections.OrderedDict()\\n    index = 0\\n    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\\n        while True:\\n            token = reader.readline()\\n            if not token:\\n                break\\n            token = token.strip()\\n            vocab[token] = index\\n            index += 1\\n    return vocab\\n\\n\\ndef whitespace_tokenize(text):\\n    \"\"\"Runs basic whitespace cleaning and splitting on a peice of text.\"\"\"\\n    text = text.strip()\\n    if not text:\\n        return []\\n    tokens = text.split()\\n    return tokens\\n\\n\\nclass BertTokenizer(object):\\n    \"\"\"Runs end-to-end tokenization: punctuation splitting + wordpiece\"\"\"\\n\\n    def __init__(self, vocab_file, do_lower_case=True, max_len=None,\\n                 never_split=(\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\")):\\n        if not os.path.isfile(vocab_file):\\n            raise ValueError(\\n                \"Can\\'t find a vocabulary file at path \\'{}\\'. To load the vocabulary from a Google pretrained \"\\n                \"model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\".format(vocab_file))\\n        self.vocab = load_vocab(vocab_file)\\n        self.ids_to_tokens = collections.OrderedDict(\\n            [(ids, tok) for tok, ids in self.vocab.items()])\\n        self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case,\\n                                              never_split=never_split)\\n        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\\n        self.max_len = max_len if max_len is not None else int(1e12)\\n\\n    def tokenize(self, text):\\n        split_tokens = []\\n        for token in self.basic_tokenizer.tokenize(text):\\n            for sub_token in self.wordpiece_tokenizer.tokenize(token):\\n                split_tokens.append(sub_token)\\n        return split_tokens\\n\\n    def convert_tokens_to_ids(self, tokens):\\n        \"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"\\n        ids = []\\n        for token in tokens:\\n            ids.append(self.vocab[token])\\n        if len(ids) > self.max_len:\\n            raise ValueError(\\n                \"Token indices sequence length is longer than the specified maximum \"\\n                \" sequence length for this BERT model ({} > {}). Running this\"\\n                \" sequence through BERT will result in indexing errors\".format(len(ids), self.max_len)\\n            )\\n        return ids\\n\\n    def convert_ids_to_tokens(self, ids):\\n        \"\"\"Converts a sequence of ids in wordpiece tokens using the vocab.\"\"\"\\n        tokens = []\\n        for i in ids:\\n            tokens.append(self.ids_to_tokens[i])\\n        return tokens\\n\\n    @classmethod\\n    def from_pretrained(cls, pretrained_model_name, cache_dir=None, *inputs, **kwargs):\\n        \"\"\"\\n        Instantiate a PreTrainedBertModel from a pre-trained model file.\\n        Download and cache the pre-trained model file if needed.\\n        \"\"\"\\n        if pretrained_model_name in PRETRAINED_VOCAB_ARCHIVE_MAP:\\n            vocab_file = PRETRAINED_VOCAB_ARCHIVE_MAP[pretrained_model_name]\\n        else:\\n            vocab_file = pretrained_model_name\\n        if os.path.isdir(vocab_file):\\n            vocab_file = os.path.join(vocab_file, VOCAB_NAME)\\n        # redirect to the cache, if necessary\\n        try:\\n            resolved_vocab_file = cached_path(vocab_file, cache_dir=cache_dir)\\n        except FileNotFoundError:\\n            logger.error(\\n                \"Model name \\'{}\\' was not found in model name list ({}). \"\\n                \"We assumed \\'{}\\' was a path or url but couldn\\'t find any file \"\\n                \"associated to this path or url.\".format(\\n                    pretrained_model_name,\\n                    \\', \\'.join(PRETRAINED_VOCAB_ARCHIVE_MAP.keys()),\\n                    vocab_file))\\n            return None\\n        if resolved_vocab_file == vocab_file:\\n            logger.info(\"loading vocabulary file {}\".format(vocab_file))\\n        else:\\n            logger.info(\"loading vocabulary file {} from cache at {}\".format(\\n                vocab_file, resolved_vocab_file))\\n        if pretrained_model_name in PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP:\\n            # if we\\'re using a pretrained model, ensure the tokenizer wont index sequences longer\\n            # than the number of positional embeddings\\n            max_len = PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP[pretrained_model_name]\\n            kwargs[\\'max_len\\'] = min(kwargs.get(\\'max_len\\', int(1e12)), max_len)\\n        # Instantiate tokenizer.\\n        tokenizer = cls(resolved_vocab_file, *inputs, **kwargs)\\n        return tokenizer\\n\\n\\nclass BasicTokenizer(object):\\n    \"\"\"Runs basic tokenization (punctuation splitting, lower casing, etc.).\"\"\"\\n\\n    def __init__(self,\\n                 do_lower_case=True,\\n                 never_split=(\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\")):\\n        \"\"\"Constructs a BasicTokenizer.\\n\\n        Args:\\n          do_lower_case: Whether to lower case the input.\\n        \"\"\"\\n        self.do_lower_case = do_lower_case\\n        self.never_split = never_split\\n\\n    def tokenize(self, text):\\n        \"\"\"Tokenizes a piece of text.\"\"\"\\n        text = self._clean_text(text)\\n        # This was added on November 1st, 2018 for the multilingual and Chinese\\n        # models. This is also applied to the English models now, but it doesn\\'t\\n        # matter since the English models were not trained on any Chinese data\\n        # and generally don\\'t have any Chinese data in them (there are Chinese\\n        # characters in the vocabulary because Wikipedia does have some Chinese\\n        # words in the English Wikipedia.).\\n        text = self._tokenize_chinese_chars(text)\\n        orig_tokens = whitespace_tokenize(text)\\n        split_tokens = []\\n        for token in orig_tokens:\\n            if self.do_lower_case and token not in self.never_split:\\n                token = token.lower()\\n                token = self._run_strip_accents(token)\\n            split_tokens.extend(self._run_split_on_punc(token))\\n\\n        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\\n        return output_tokens\\n\\n    def _run_strip_accents(self, text):\\n        \"\"\"Strips accents from a piece of text.\"\"\"\\n        text = unicodedata.normalize(\"NFD\", text)\\n        output = []\\n        for char in text:\\n            cat = unicodedata.category(char)\\n            if cat == \"Mn\":\\n                continue\\n            output.append(char)\\n        return \"\".join(output)\\n\\n    def _run_split_on_punc(self, text):\\n        \"\"\"Splits punctuation on a piece of text.\"\"\"\\n        if text in self.never_split:\\n            return [text]\\n        chars = list(text)\\n        i = 0\\n        start_new_word = True\\n        output = []\\n        while i < len(chars):\\n            char = chars[i]\\n            if _is_punctuation(char):\\n                output.append([char])\\n                start_new_word = True\\n            else:\\n                if start_new_word:\\n                    output.append([])\\n                start_new_word = False\\n                output[-1].append(char)\\n            i += 1\\n\\n        return [\"\".join(x) for x in output]\\n\\n    def _tokenize_chinese_chars(self, text):\\n        \"\"\"Adds whitespace around any CJK character.\"\"\"\\n        output = []\\n        for char in text:\\n            cp = ord(char)\\n            if self._is_chinese_char(cp):\\n                output.append(\" \")\\n                output.append(char)\\n                output.append(\" \")\\n            else:\\n                output.append(char)\\n        return \"\".join(output)\\n\\n    def _is_chinese_char(self, cp):\\n        \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\\n        # This defines a \"chinese character\" as anything in the CJK Unicode block:\\n        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\\n        #\\n        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\\n        # despite its name. The modern Korean Hangul alphabet is a different block,\\n        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\\n        # space-separated words, so they are not treated specially and handled\\n        # like the all of the other languages.\\n        if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\\n                (cp >= 0x3400 and cp <= 0x4DBF) or  #\\n                (cp >= 0x20000 and cp <= 0x2A6DF) or  #\\n                (cp >= 0x2A700 and cp <= 0x2B73F) or  #\\n                (cp >= 0x2B740 and cp <= 0x2B81F) or  #\\n                (cp >= 0x2B820 and cp <= 0x2CEAF) or\\n                (cp >= 0xF900 and cp <= 0xFAFF) or  #\\n                (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\\n            return True\\n\\n        return False\\n\\n    def _clean_text(self, text):\\n        \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\\n        output = []\\n        for char in text:\\n            cp = ord(char)\\n            if cp == 0 or cp == 0xfffd or _is_control(char):\\n                continue\\n            if _is_whitespace(char):\\n                output.append(\" \")\\n            else:\\n                output.append(char)\\n        return \"\".join(output)\\n\\n\\nclass WordpieceTokenizer(object):\\n    \"\"\"Runs WordPiece tokenization.\"\"\"\\n\\n    def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=100):\\n        self.vocab = vocab\\n        self.unk_token = unk_token\\n        self.max_input_chars_per_word = max_input_chars_per_word\\n\\n    def tokenize(self, text):\\n        \"\"\"Tokenizes a piece of text into its word pieces.\\n\\n        This uses a greedy longest-match-first algorithm to perform tokenization\\n        using the given vocabulary.\\n\\n        For example:\\n          input = \"unaffable\"\\n          output = [\"un\", \"##aff\", \"##able\"]\\n\\n        Args:\\n          text: A single token or whitespace separated tokens. This should have\\n            already been passed through `BasicTokenizer`.\\n\\n        Returns:\\n          A list of wordpiece tokens.\\n        \"\"\"\\n\\n        output_tokens = []\\n        for token in whitespace_tokenize(text):\\n            chars = list(token)\\n            if len(chars) > self.max_input_chars_per_word:\\n                output_tokens.append(self.unk_token)\\n                continue\\n\\n            is_bad = False\\n            start = 0\\n            sub_tokens = []\\n            while start < len(chars):\\n                end = len(chars)\\n                cur_substr = None\\n                while start < end:\\n                    substr = \"\".join(chars[start:end])\\n                    if start > 0:\\n                        substr = \"##\" + substr\\n                    if substr in self.vocab:\\n                        cur_substr = substr\\n                        break\\n                    end -= 1\\n                if cur_substr is None:\\n                    is_bad = True\\n                    break\\n                sub_tokens.append(cur_substr)\\n                start = end\\n\\n            if is_bad:\\n                output_tokens.append(self.unk_token)\\n            else:\\n                output_tokens.extend(sub_tokens)\\n        return output_tokens\\n\\n\\ndef _is_whitespace(char):\\n    \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\\n    # \\\\t, \\\\n, and \\\\r are technically contorl characters but we treat them\\n    # as whitespace since they are generally considered as such.\\n    if char == \" \" or char == \"\\\\t\" or char == \"\\\\n\" or char == \"\\\\r\":\\n        return True\\n    cat = unicodedata.category(char)\\n    if cat == \"Zs\":\\n        return True\\n    return False\\n\\n\\ndef _is_control(char):\\n    \"\"\"Checks whether `chars` is a control character.\"\"\"\\n    # These are technically control characters but we count them as whitespace\\n    # characters.\\n    if char == \"\\\\t\" or char == \"\\\\n\" or char == \"\\\\r\":\\n        return False\\n    cat = unicodedata.category(char)\\n    if cat.startswith(\"C\"):\\n        return True\\n    return False\\n\\n\\ndef _is_punctuation(char):\\n    \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\\n    cp = ord(char)\\n    # We treat all non-letter/number ASCII as punctuation.\\n    # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\\n    # Punctuation class but we treat them as punctuation anyways, for\\n    # consistency.\\n    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\\n            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\\n        return True\\n    cat = unicodedata.category(char)\\n    if cat.startswith(\"P\"):\\n        return True\\n    return False\\n'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "1U-M-Bb9Sx3O",
        "colab_type": "code",
        "outputId": "df79137f-ecda-49eb-81f9-17da972ca26f",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "cell_type": "code",
      "source": [
        "# Upload optimization.py from local drive\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bc6f7547-c105-466b-925f-fe2b0432c9b1\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-bc6f7547-c105-466b-925f-fe2b0432c9b1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving optimization.py to optimization (2).py\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'optimization.py': b'# coding=utf-8\\n# Copyright 2018 The Google AI Language Team Authors and The HugginFace Inc. team.\\n#\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\"\"\"PyTorch optimization for BERT model.\"\"\"\\n\\nimport math\\nimport torch\\nfrom torch.optim import Optimizer\\nfrom torch.optim.optimizer import required\\nfrom torch.nn.utils import clip_grad_norm_\\n\\ndef warmup_cosine(x, warmup=0.002):\\n    if x < warmup:\\n        return x/warmup\\n    return 0.5 * (1.0 + torch.cos(math.pi * x))\\n\\ndef warmup_constant(x, warmup=0.002):\\n    if x < warmup:\\n        return x/warmup\\n    return 1.0\\n\\ndef warmup_linear(x, warmup=0.002):\\n    if x < warmup:\\n        return x/warmup\\n    return 1.0 - x\\n\\nSCHEDULES = {\\n    \\'warmup_cosine\\':warmup_cosine,\\n    \\'warmup_constant\\':warmup_constant,\\n    \\'warmup_linear\\':warmup_linear,\\n}\\n\\n\\nclass BertAdam(Optimizer):\\n    \"\"\"Implements BERT version of Adam algorithm with weight decay fix.\\n    Params:\\n        lr: learning rate\\n        warmup: portion of t_total for the warmup, -1  means no warmup. Default: -1\\n        t_total: total number of training steps for the learning\\n            rate schedule, -1  means constant learning rate. Default: -1\\n        schedule: schedule to use for the warmup (see above). Default: \\'warmup_linear\\'\\n        b1: Adams b1. Default: 0.9\\n        b2: Adams b2. Default: 0.999\\n        e: Adams epsilon. Default: 1e-6\\n        weight_decay: Weight decay. Default: 0.01\\n        max_grad_norm: Maximum norm for the gradients (-1 means no clipping). Default: 1.0\\n    \"\"\"\\n    def __init__(self, params, lr=required, warmup=-1, t_total=-1, schedule=\\'warmup_linear\\',\\n                 b1=0.9, b2=0.999, e=1e-6, weight_decay=0.01,\\n                 max_grad_norm=1.0):\\n        if lr is not required and lr < 0.0:\\n            raise ValueError(\"Invalid learning rate: {} - should be >= 0.0\".format(lr))\\n        if schedule not in SCHEDULES:\\n            raise ValueError(\"Invalid schedule parameter: {}\".format(schedule))\\n        if not 0.0 <= warmup < 1.0 and not warmup == -1:\\n            raise ValueError(\"Invalid warmup: {} - should be in [0.0, 1.0[ or -1\".format(warmup))\\n        if not 0.0 <= b1 < 1.0:\\n            raise ValueError(\"Invalid b1 parameter: {} - should be in [0.0, 1.0[\".format(b1))\\n        if not 0.0 <= b2 < 1.0:\\n            raise ValueError(\"Invalid b2 parameter: {} - should be in [0.0, 1.0[\".format(b2))\\n        if not e >= 0.0:\\n            raise ValueError(\"Invalid epsilon value: {} - should be >= 0.0\".format(e))\\n        defaults = dict(lr=lr, schedule=schedule, warmup=warmup, t_total=t_total,\\n                        b1=b1, b2=b2, e=e, weight_decay=weight_decay,\\n                        max_grad_norm=max_grad_norm)\\n        super(BertAdam, self).__init__(params, defaults)\\n\\n    def get_lr(self):\\n        lr = []\\n        for group in self.param_groups:\\n            for p in group[\\'params\\']:\\n                state = self.state[p]\\n                if len(state) == 0:\\n                    return [0]\\n                if group[\\'t_total\\'] != -1:\\n                    schedule_fct = SCHEDULES[group[\\'schedule\\']]\\n                    lr_scheduled = group[\\'lr\\'] * schedule_fct(state[\\'step\\']/group[\\'t_total\\'], group[\\'warmup\\'])\\n                else:\\n                    lr_scheduled = group[\\'lr\\']\\n                lr.append(lr_scheduled)\\n        return lr\\n\\n    def step(self, closure=None):\\n        \"\"\"Performs a single optimization step.\\n\\n        Arguments:\\n            closure (callable, optional): A closure that reevaluates the model\\n                and returns the loss.\\n        \"\"\"\\n        loss = None\\n        if closure is not None:\\n            loss = closure()\\n\\n        for group in self.param_groups:\\n            for p in group[\\'params\\']:\\n                if p.grad is None:\\n                    continue\\n                grad = p.grad.data\\n                if grad.is_sparse:\\n                    raise RuntimeError(\\'Adam does not support sparse gradients, please consider SparseAdam instead\\')\\n\\n                state = self.state[p]\\n\\n                # State initialization\\n                if len(state) == 0:\\n                    state[\\'step\\'] = 0\\n                    # Exponential moving average of gradient values\\n                    state[\\'next_m\\'] = torch.zeros_like(p.data)\\n                    # Exponential moving average of squared gradient values\\n                    state[\\'next_v\\'] = torch.zeros_like(p.data)\\n\\n                next_m, next_v = state[\\'next_m\\'], state[\\'next_v\\']\\n                beta1, beta2 = group[\\'b1\\'], group[\\'b2\\']\\n\\n                # Add grad clipping\\n                if group[\\'max_grad_norm\\'] > 0:\\n                    clip_grad_norm_(p, group[\\'max_grad_norm\\'])\\n\\n                # Decay the first and second moment running average coefficient\\n                # In-place operations to update the averages at the same time\\n                next_m.mul_(beta1).add_(1 - beta1, grad)\\n                next_v.mul_(beta2).addcmul_(1 - beta2, grad, grad)\\n                update = next_m / (next_v.sqrt() + group[\\'e\\'])\\n\\n                # Just adding the square of the weights to the loss function is *not*\\n                # the correct way of using L2 regularization/weight decay with Adam,\\n                # since that will interact with the m and v parameters in strange ways.\\n                #\\n                # Instead we want to decay the weights in a manner that doesn\\'t interact\\n                # with the m/v parameters. This is equivalent to adding the square\\n                # of the weights to the loss with plain (non-momentum) SGD.\\n                if group[\\'weight_decay\\'] > 0.0:\\n                    update += group[\\'weight_decay\\'] * p.data\\n\\n                if group[\\'t_total\\'] != -1:\\n                    schedule_fct = SCHEDULES[group[\\'schedule\\']]\\n                    lr_scheduled = group[\\'lr\\'] * schedule_fct(state[\\'step\\']/group[\\'t_total\\'], group[\\'warmup\\'])\\n                else:\\n                    lr_scheduled = group[\\'lr\\']\\n\\n                update_with_lr = lr_scheduled * update\\n                p.data.add_(-update_with_lr)\\n\\n                state[\\'step\\'] += 1\\n\\n                # step_size = lr_scheduled * math.sqrt(bias_correction2) / bias_correction1\\n                # No bias correction\\n                # bias_correction1 = 1 - beta1 ** state[\\'step\\']\\n                # bias_correction2 = 1 - beta2 ** state[\\'step\\']\\n\\n        return loss\\n'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "iYZ7azXNTmNq",
        "colab_type": "code",
        "outputId": "d5cda447-3178-4a1c-91b0-54ed184d3074",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "cell_type": "code",
      "source": [
        "# Upload file_util.py from local drive\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5df44eb0-ca94-4269-983e-26e027bb2d0f\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-5df44eb0-ca94-4269-983e-26e027bb2d0f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving file_utils.py to file_utils (2).py\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'file_utils.py': b'\"\"\"\\nUtilities for working with the local dataset cache.\\nThis file is adapted from the AllenNLP library at https://github.com/allenai/allennlp\\nCopyright by the AllenNLP authors.\\n\"\"\"\\nfrom __future__ import (absolute_import, division, print_function, unicode_literals)\\n\\nimport json\\nimport logging\\nimport os\\nimport shutil\\nimport tempfile\\nfrom functools import wraps\\nfrom hashlib import sha256\\nimport sys\\nfrom io import open\\n\\nimport boto3\\nimport requests\\nfrom botocore.exceptions import ClientError\\nfrom tqdm import tqdm\\n\\ntry:\\n    from urllib.parse import urlparse\\nexcept ImportError:\\n    from urlparse import urlparse\\n\\ntry:\\n    from pathlib import Path\\n    PYTORCH_PRETRAINED_BERT_CACHE = Path(os.getenv(\\'PYTORCH_PRETRAINED_BERT_CACHE\\',\\n                                                   Path.home() / \\'.pytorch_pretrained_bert\\'))\\nexcept AttributeError:\\n    PYTORCH_PRETRAINED_BERT_CACHE = os.getenv(\\'PYTORCH_PRETRAINED_BERT_CACHE\\',\\n                                              os.path.join(os.path.expanduser(\"~\"), \\'.pytorch_pretrained_bert\\'))\\n\\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\\n\\n\\ndef url_to_filename(url, etag=None):\\n    \"\"\"\\n    Convert `url` into a hashed filename in a repeatable way.\\n    If `etag` is specified, append its hash to the url\\'s, delimited\\n    by a period.\\n    \"\"\"\\n    url_bytes = url.encode(\\'utf-8\\')\\n    url_hash = sha256(url_bytes)\\n    filename = url_hash.hexdigest()\\n\\n    if etag:\\n        etag_bytes = etag.encode(\\'utf-8\\')\\n        etag_hash = sha256(etag_bytes)\\n        filename += \\'.\\' + etag_hash.hexdigest()\\n\\n    return filename\\n\\n\\ndef filename_to_url(filename, cache_dir=None):\\n    \"\"\"\\n    Return the url and etag (which may be ``None``) stored for `filename`.\\n    Raise ``EnvironmentError`` if `filename` or its stored metadata do not exist.\\n    \"\"\"\\n    if cache_dir is None:\\n        cache_dir = PYTORCH_PRETRAINED_BERT_CACHE\\n    if sys.version_info[0] == 3 and isinstance(cache_dir, Path):\\n        cache_dir = str(cache_dir)\\n\\n    cache_path = os.path.join(cache_dir, filename)\\n    if not os.path.exists(cache_path):\\n        raise EnvironmentError(\"file {} not found\".format(cache_path))\\n\\n    meta_path = cache_path + \\'.json\\'\\n    if not os.path.exists(meta_path):\\n        raise EnvironmentError(\"file {} not found\".format(meta_path))\\n\\n    with open(meta_path, encoding=\"utf-8\") as meta_file:\\n        metadata = json.load(meta_file)\\n    url = metadata[\\'url\\']\\n    etag = metadata[\\'etag\\']\\n\\n    return url, etag\\n\\n\\ndef cached_path(url_or_filename, cache_dir=None):\\n    \"\"\"\\n    Given something that might be a URL (or might be a local path),\\n    determine which. If it\\'s a URL, download the file and cache it, and\\n    return the path to the cached file. If it\\'s already a local path,\\n    make sure the file exists and then return the path.\\n    \"\"\"\\n    if cache_dir is None:\\n        cache_dir = PYTORCH_PRETRAINED_BERT_CACHE\\n    if sys.version_info[0] == 3 and isinstance(url_or_filename, Path):\\n        url_or_filename = str(url_or_filename)\\n    if sys.version_info[0] == 3 and isinstance(cache_dir, Path):\\n        cache_dir = str(cache_dir)\\n\\n    parsed = urlparse(url_or_filename)\\n\\n    if parsed.scheme in (\\'http\\', \\'https\\', \\'s3\\'):\\n        # URL, so get it from the cache (downloading if necessary)\\n        return get_from_cache(url_or_filename, cache_dir)\\n    elif os.path.exists(url_or_filename):\\n        # File, and it exists.\\n        return url_or_filename\\n    elif parsed.scheme == \\'\\':\\n        # File, but it doesn\\'t exist.\\n        raise EnvironmentError(\"file {} not found\".format(url_or_filename))\\n    else:\\n        # Something unknown\\n        raise ValueError(\"unable to parse {} as a URL or as a local path\".format(url_or_filename))\\n\\n\\ndef split_s3_path(url):\\n    \"\"\"Split a full s3 path into the bucket name and path.\"\"\"\\n    parsed = urlparse(url)\\n    if not parsed.netloc or not parsed.path:\\n        raise ValueError(\"bad s3 path {}\".format(url))\\n    bucket_name = parsed.netloc\\n    s3_path = parsed.path\\n    # Remove \\'/\\' at beginning of path.\\n    if s3_path.startswith(\"/\"):\\n        s3_path = s3_path[1:]\\n    return bucket_name, s3_path\\n\\n\\ndef s3_request(func):\\n    \"\"\"\\n    Wrapper function for s3 requests in order to create more helpful error\\n    messages.\\n    \"\"\"\\n\\n    @wraps(func)\\n    def wrapper(url, *args, **kwargs):\\n        try:\\n            return func(url, *args, **kwargs)\\n        except ClientError as exc:\\n            if int(exc.response[\"Error\"][\"Code\"]) == 404:\\n                raise EnvironmentError(\"file {} not found\".format(url))\\n            else:\\n                raise\\n\\n    return wrapper\\n\\n\\n@s3_request\\ndef s3_etag(url):\\n    \"\"\"Check ETag on S3 object.\"\"\"\\n    s3_resource = boto3.resource(\"s3\")\\n    bucket_name, s3_path = split_s3_path(url)\\n    s3_object = s3_resource.Object(bucket_name, s3_path)\\n    return s3_object.e_tag\\n\\n\\n@s3_request\\ndef s3_get(url, temp_file):\\n    \"\"\"Pull a file directly from S3.\"\"\"\\n    s3_resource = boto3.resource(\"s3\")\\n    bucket_name, s3_path = split_s3_path(url)\\n    s3_resource.Bucket(bucket_name).download_fileobj(s3_path, temp_file)\\n\\n\\ndef http_get(url, temp_file):\\n    req = requests.get(url, stream=True)\\n    content_length = req.headers.get(\\'Content-Length\\')\\n    total = int(content_length) if content_length is not None else None\\n    progress = tqdm(unit=\"B\", total=total)\\n    for chunk in req.iter_content(chunk_size=1024):\\n        if chunk: # filter out keep-alive new chunks\\n            progress.update(len(chunk))\\n            temp_file.write(chunk)\\n    progress.close()\\n\\n\\ndef get_from_cache(url, cache_dir=None):\\n    \"\"\"\\n    Given a URL, look for the corresponding dataset in the local cache.\\n    If it\\'s not there, download it. Then return the path to the cached file.\\n    \"\"\"\\n    if cache_dir is None:\\n        cache_dir = PYTORCH_PRETRAINED_BERT_CACHE\\n    if sys.version_info[0] == 3 and isinstance(cache_dir, Path):\\n        cache_dir = str(cache_dir)\\n\\n    if not os.path.exists(cache_dir):\\n        os.makedirs(cache_dir)\\n\\n    # Get eTag to add to filename, if it exists.\\n    if url.startswith(\"s3://\"):\\n        etag = s3_etag(url)\\n    else:\\n        response = requests.head(url, allow_redirects=True)\\n        if response.status_code != 200:\\n            raise IOError(\"HEAD request failed for url {} with status code {}\"\\n                          .format(url, response.status_code))\\n        etag = response.headers.get(\"ETag\")\\n\\n    filename = url_to_filename(url, etag)\\n\\n    # get cache path to put the file\\n    cache_path = os.path.join(cache_dir, filename)\\n\\n    if not os.path.exists(cache_path):\\n        # Download to temporary file, then copy to cache dir once finished.\\n        # Otherwise you get corrupt cache entries if the download gets interrupted.\\n        with tempfile.NamedTemporaryFile() as temp_file:\\n            logger.info(\"%s not found in cache, downloading to %s\", url, temp_file.name)\\n\\n            # GET file object\\n            if url.startswith(\"s3://\"):\\n                s3_get(url, temp_file)\\n            else:\\n                http_get(url, temp_file)\\n\\n            # we are copying the file before closing it, so flush to avoid truncation\\n            temp_file.flush()\\n            # shutil.copyfileobj() starts at the current position, so go to the start\\n            temp_file.seek(0)\\n\\n            logger.info(\"copying %s to cache at %s\", temp_file.name, cache_path)\\n            with open(cache_path, \\'wb\\') as cache_file:\\n                shutil.copyfileobj(temp_file, cache_file)\\n\\n            logger.info(\"creating metadata file for %s\", cache_path)\\n            meta = {\\'url\\': url, \\'etag\\': etag}\\n            meta_path = cache_path + \\'.json\\'\\n            with open(meta_path, \\'w\\', encoding=\"utf-8\") as meta_file:\\n                json.dump(meta, meta_file)\\n\\n            logger.info(\"removing temp file %s\", temp_file.name)\\n\\n    return cache_path\\n\\n\\ndef read_set_from_file(filename):\\n    \\'\\'\\'\\n    Extract a de-duped collection (set) of text from a file.\\n    Expected file format is one item per line.\\n    \\'\\'\\'\\n    collection = set()\\n    with open(filename, \\'r\\', encoding=\\'utf-8\\') as file_:\\n        for line in file_:\\n            collection.add(line.rstrip())\\n    return collection\\n\\n\\ndef get_file_extension(path, dot=True, lower=True):\\n    ext = os.path.splitext(path)[1]\\n    ext = ext if dot else ext[1:]\\n    return ext.lower() if lower else ext\\n'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "heading_collapsed": true,
        "id": "J5opMq5iAPmQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ]
    },
    {
      "metadata": {
        "hidden": true,
        "id": "7aY0237CAPmQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from modeling import PreTrainedBertModel, BertModel\n",
        "from tokenization import BertTokenizer\n",
        "from optimization import BertAdam, warmup_linear, SCHEDULES\n",
        "from fastprogress import master_bar, progress_bar\n",
        "from sklearn.model_selection import StratifiedShuffleSplit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "hidden": true,
        "id": "Wpo46ZvIAPmW",
        "colab_type": "code",
        "outputId": "bcb77a7c-af7b-4804-8cd6-ea2d52386c9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "logger.info(\"device: {} n_gpu: {}, 16-bits training: {}\".format(\n",
        "    device, n_gpu, FP16))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "03/01/2019 00:43:28 - INFO - regressor -   device: cuda n_gpu: 1, 16-bits training: False\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "hidden": true,
        "id": "5IXa8whhAPma",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Set random seeds:"
      ]
    },
    {
      "metadata": {
        "hidden": true,
        "id": "JM-4QtcoAPmb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if n_gpu > 0:\n",
        "    torch.cuda.manual_seed_all(SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uslhJA8SAPme",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Definitions"
      ]
    },
    {
      "metadata": {
        "id": "KccrlI48APme",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Regression model:"
      ]
    },
    {
      "metadata": {
        "id": "CqveiW1iAPmf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BertForSequenceRegression(PreTrainedBertModel):\n",
        "    def __init__(self, config):\n",
        "        super(BertForSequenceRegression, self).__init__(config)\n",
        "        self.bert = BertModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.regressor = nn.Linear(config.hidden_size, 1)\n",
        "        self.apply(self.init_bert_weights)\n",
        "        self.loss_fct = torch.nn.MSELoss()\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, targets=None):\n",
        "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        outputs = self.regressor(pooled_output).clamp(-1, 1)\n",
        "        if targets is not None:\n",
        "            loss = self.loss_fct(outputs.view(-1), targets.view(-1))\n",
        "            return loss\n",
        "        else:\n",
        "            return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z4Z88F1GAPmh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Data Classes:"
      ]
    },
    {
      "metadata": {
        "id": "aENegWjXAPmj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class InputExample(object):\n",
        "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "    def __init__(self, guid, text, target=None):\n",
        "        self.guid = guid\n",
        "        self.text = text\n",
        "        self.target = target\n",
        "\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, input_mask, segment_ids, target):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.target = target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xLpzcgSrAPml",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Data Processing Class and Function:"
      ]
    },
    {
      "metadata": {
        "id": "5YL0FOsAAPml",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DoubanRatingProcessor:\n",
        "    \"\"\"Processor for the Douban movie ratings data set.\"\"\"\n",
        "    def __init__(self, sample_ratio: float = 0.05):\n",
        "        df_ratings = self.filter_entries(pd.read_csv(DATA_PATH)).sample(frac=sample_ratio)\n",
        "        df_ratings[\"rating\"] = ((df_ratings[\"rating\"] - 3) / 2).astype(\"float32\")\n",
        "        assert df_ratings.rating.max() <= 1\n",
        "        assert df_ratings.rating.min() >= -1\n",
        "        assert df_ratings.isnull().sum().sum() == 0\n",
        "        texts = df_ratings[\"comment\"].values\n",
        "        # Split the dataset\n",
        "        sss = StratifiedShuffleSplit(n_splits=1, test_size=0.4, random_state=888)\n",
        "        train_idx, test_idx = next(sss.split(df_ratings, df_ratings.rating))\n",
        "        texts_train, texts_test = texts[train_idx], texts[test_idx]\n",
        "        y_train = df_ratings.iloc[train_idx][[\"rating\"]].copy().values\n",
        "        y_test = df_ratings.iloc[test_idx][[\"rating\"]].copy().values\n",
        "        sss = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=888)\n",
        "        val_idx, test_idx = next(sss.split(y_test, y_test))\n",
        "        texts_valid, texts_test = texts_test[val_idx], texts_test[test_idx]\n",
        "        y_valid, y_test = y_test[val_idx], y_test[test_idx]\n",
        "        self.x_train, self.x_valid, self.x_test = (\n",
        "            texts_train, texts_valid, texts_test)\n",
        "        self.y_train, self.y_valid, self.y_test = (\n",
        "            y_train, y_valid, y_test)\n",
        "\n",
        "    @classmethod\n",
        "    def filter_entries(cls, df_ratings, min_len=3, max_len=1000):\n",
        "        lengths = df_ratings.comment.str.len()\n",
        "        flags = (lengths >= min_len) & (lengths <= max_len)\n",
        "        assert flags.isnull().sum() == 0\n",
        "        return df_ratings.loc[flags].copy()\n",
        "        \n",
        "    def get_train_examples(self):\n",
        "        return self._create_examples(self.x_train, self.y_train)\n",
        "\n",
        "    def get_dev_examples(self):\n",
        "        return self._create_examples(self.x_valid, self.y_valid)\n",
        "\n",
        "    def get_test_examples(self):\n",
        "        return self._create_examples(self.x_test, self.y_test)\n",
        "    \n",
        "    def _create_examples(self, x, y):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "        for (i, (texts, target)) in enumerate(zip(x, y)):\n",
        "            examples.append(\n",
        "                InputExample(guid=i, text=texts, target=target))\n",
        "        return examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W7fSEXboAPmo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def convert_examples_to_features(examples, max_seq_length, tokenizer):\n",
        "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "    \n",
        "    features = []\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        tokens = tokenizer.tokenize(example.text)\n",
        "        \n",
        "        if len(tokens) > max_seq_length - 2:\n",
        "            tokens = tokens[:(max_seq_length - 2)]\n",
        "\n",
        "        # The convention in BERT is:\n",
        "        # (a) For sequence pairs:\n",
        "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
        "        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
        "        # (b) For single sequences:\n",
        "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
        "        #  type_ids: 0   0   0   0  0     0 0\n",
        "        #\n",
        "        # Where \"type_ids\" are used to indicate whether this is the first\n",
        "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
        "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
        "        # embedding vector (and position vector). This is not *strictly* necessary\n",
        "        # since the [SEP] token unambigiously separates the sequences, but it makes\n",
        "        # it easier for the model to learn the concept of sequences.\n",
        "        #\n",
        "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
        "        # the entire model is fine-tuned.\n",
        "        tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
        "        segment_ids = [0] * len(tokens)\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        input_mask = [1] * len(input_ids)\n",
        "\n",
        "        # Zero-pad up to the sequence length.\n",
        "        padding = [0] * (max_seq_length - len(input_ids))\n",
        "        input_ids += padding\n",
        "        input_mask += padding\n",
        "        segment_ids += padding\n",
        "\n",
        "        assert len(input_ids) == max_seq_length\n",
        "        assert len(input_mask) == max_seq_length\n",
        "        assert len(segment_ids) == max_seq_length\n",
        "\n",
        "        if ex_index < 5:\n",
        "            logger.info(\"*** Example ***\")\n",
        "            logger.info(\"guid: %s\" % (example.guid))\n",
        "            logger.info(\"tokens: %s\" % \" \".join(\n",
        "                    [str(x) for x in tokens]))\n",
        "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "            logger.info(\n",
        "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
        "            logger.info(\"target: %s\" % (example.target))\n",
        "\n",
        "        features.append(\n",
        "                InputFeatures(input_ids=input_ids,\n",
        "                              input_mask=input_mask,\n",
        "                              segment_ids=segment_ids,\n",
        "                              target=example.target))\n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nh8hNE7CAPmp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class FreezableBertAdam(BertAdam):\n",
        "    def get_lr(self):\n",
        "        lr = []\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                state = self.state[p]\n",
        "                if len(state) == 0:\n",
        "                    continue\n",
        "                if group['t_total'] != -1:\n",
        "                    schedule_fct = SCHEDULES[group['schedule']]\n",
        "                    lr_scheduled = group['lr'] * schedule_fct(state['step']/group['t_total'], group['warmup'])\n",
        "                else:\n",
        "                    lr_scheduled = group['lr']\n",
        "                lr.append(lr_scheduled)\n",
        "        return lr    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sDo0GhX6APmr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Utility functions:"
      ]
    },
    {
      "metadata": {
        "id": "e7FFChFOAPms",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def children(m):\n",
        "    return m if isinstance(m, (list, tuple)) else list(m.children())\n",
        "\n",
        "\n",
        "def set_trainable_attr(m, b):\n",
        "    m.trainable = b\n",
        "    for p in m.parameters():\n",
        "        p.requires_grad = b\n",
        "\n",
        "\n",
        "def apply_leaf(m, f):\n",
        "    c = children(m)\n",
        "    if isinstance(m, nn.Module):\n",
        "        f(m)\n",
        "    if len(c) > 0:\n",
        "        for l in c:\n",
        "            apply_leaf(l, f)\n",
        "\n",
        "\n",
        "def set_trainable(l, b):\n",
        "    apply_leaf(l, lambda m: set_trainable_attr(m, b))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DFwu6je3APmu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def count_model_parameters(model):\n",
        "    logger.info(\n",
        "        \"# of paramters: {:,d}\".format(\n",
        "            sum(p.numel() for p in model.parameters())))\n",
        "    logger.info(\n",
        "        \"# of trainable paramters: {:,d}\".format(\n",
        "            sum(p.numel() for p in model.parameters() if p.requires_grad)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vKRrT7zdAPmw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training"
      ]
    },
    {
      "metadata": {
        "heading_collapsed": true,
        "id": "7ckpEQrJAPmx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "metadata": {
        "hidden": true,
        "id": "E-w6nlqLAPmx",
        "colab_type": "code",
        "outputId": "42c880c3-e96e-4238-8deb-bd403c91ce77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2275
        }
      },
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\n",
        "    \"bert-base-chinese\", do_lower_case=True, \n",
        "    cache_dir=PYTORCH_PRETRAINED_BERT_CACHE)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ProxyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    140\u001b[0m             conn = connection.create_connection(\n\u001b[0;32m--> 141\u001b[0;31m                 (self.host, self.port), self.timeout, **extra_kw)\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    594\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_new_proxy_conn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_proxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_prepare_proxy\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m         \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;31m# Add certificate verification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    149\u001b[0m             raise NewConnectionError(\n\u001b[0;32m--> 150\u001b[0;31m                 self, \"Failed to establish a new connection: %s\" % e)\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.VerifiedHTTPSConnection object at 0x7f434624e240>: Failed to establish a new connection: [Errno 111] Connection refused",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    439\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    638\u001b[0m             retries = retries.increment(method, url, error=e, _pool=self,\n\u001b[0;32m--> 639\u001b[0;31m                                         _stacktrace=sys.exc_info()[2])\n\u001b[0m\u001b[1;32m    640\u001b[0m             \u001b[0mretries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='s3.amazonaws.com', port=443): Max retries exceeded with url: /models.huggingface.co/bert/bert-base-chinese-vocab.txt (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f434624e240>: Failed to establish a new connection: [Errno 111] Connection refused',)))",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mProxyError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-521654f44899>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m tokenizer = BertTokenizer.from_pretrained(\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"bert-base-chinese\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_lower_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     cache_dir=PYTORCH_PRETRAINED_BERT_CACHE)\n\u001b[0m",
            "\u001b[0;32m/content/tokenization.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name, cache_dir, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;31m# redirect to the cache, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0mresolved_vocab_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             logger.error(\n",
            "\u001b[0;32m/content/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mparsed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'http'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'https'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m's3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# URL, so get it from the cache (downloading if necessary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;31m# File, and it exists.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0metag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms3_etag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             raise IOError(\"HEAD request failed for url {} with status code {}\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mhead\u001b[0;34m(url, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'head'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    506\u001b[0m         }\n\u001b[1;32m    507\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ProxyError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mProxyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_SSLError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mProxyError\u001b[0m: HTTPSConnectionPool(host='s3.amazonaws.com', port=443): Max retries exceeded with url: /models.huggingface.co/bert/bert-base-chinese-vocab.txt (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f434624e240>: Failed to establish a new connection: [Errno 111] Connection refused',)))"
          ]
        }
      ]
    },
    {
      "metadata": {
        "hidden": true,
        "id": "F_SS9DLCAPm1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_examples = DoubanRatingProcessor().get_train_examples()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "hidden": true,
        "scrolled": true,
        "id": "ZT_9CuraAPm4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_features = convert_examples_to_features(\n",
        "    train_examples, MAX_SEQ_LENGTH, tokenizer)\n",
        "del train_examples\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yn9i5fDHAPm7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Model definition"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "VEJ4cccIAPm8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Prepare model\n",
        "model = BertForSequenceRegression.from_pretrained(\n",
        "    \"bert-base-chinese\",\n",
        "    cache_dir=PYTORCH_PRETRAINED_BERT_CACHE)\n",
        "if FP16:\n",
        "    model.half()\n",
        "model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KbOJe90hAPm-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Prepare optimizer\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JxkWI73NAPnA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_optimizer(num_train_optimization_steps: int, learning_rate: float):\n",
        "    grouped_parameters = [\n",
        "       x for x in optimizer_grouped_parameters if any([p.requires_grad for p in x[\"params\"]])\n",
        "    ]\n",
        "    for group in grouped_parameters:\n",
        "        group['lr'] = learning_rate\n",
        "    if FP16:\n",
        "        try:\n",
        "            from apex.optimizers import FP16_Optimizer\n",
        "            from apex.optimizers import FusedAdam\n",
        "        except ImportError:\n",
        "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex \"\n",
        "                              \"to use distributed and fp16 training.\")\n",
        "\n",
        "        optimizer = FusedAdam(grouped_parameters,\n",
        "                              lr=learning_rate, bias_correction=False,\n",
        "                              max_grad_norm=1.0)\n",
        "        if args.loss_scale == 0:\n",
        "            optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True)\n",
        "        else:\n",
        "            optimizer = FP16_Optimizer(optimizer, static_loss_scale=LOSS_SCALE)\n",
        "\n",
        "    else:\n",
        "        optimizer = FreezableBertAdam(grouped_parameters,\n",
        "                             lr=learning_rate, warmup=WARMUP_PROPORTION,\n",
        "                             t_total=num_train_optimization_steps)\n",
        "    return optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QZAgzFv0APnC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The Training Loop"
      ]
    },
    {
      "metadata": {
        "id": "U4A95MmGAPnD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(model: nn.Module, num_epochs: int, learning_rate: float):\n",
        "    num_train_optimization_steps = len(train_dataloader) * num_epochs \n",
        "    optimizer = get_optimizer(num_train_optimization_steps, learning_rate)\n",
        "    assert all([x[\"lr\"] == learning_rate for x in optimizer.param_groups])\n",
        "    global_step = 0\n",
        "    nb_tr_steps = 0\n",
        "    tr_loss = 0\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(\"  Num examples = %d\", len(train_features))\n",
        "    logger.info(\"  Batch size = %d\", BATCH_SIZE)\n",
        "    logger.info(\"  Num steps = %d\", num_train_optimization_steps)    \n",
        "    model.train()\n",
        "    mb = master_bar(range(num_epochs))\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0    \n",
        "    for _ in mb:\n",
        "        for step, batch in enumerate(progress_bar(train_dataloader, parent=mb)):\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            input_ids, input_mask, segment_ids, target = batch\n",
        "            loss = model(input_ids, segment_ids, input_mask, target)\n",
        "            if n_gpu > 1:\n",
        "                loss = loss.mean() # mean() to average on multi-gpu.\n",
        "\n",
        "            if FP16:\n",
        "                optimizer.backward(loss)\n",
        "            else:\n",
        "                loss.backward()\n",
        "\n",
        "            if tr_loss == 0:\n",
        "                tr_loss = loss.item()\n",
        "            else:\n",
        "                tr_loss = tr_loss * 0.9 + loss.item() * 0.1\n",
        "            nb_tr_examples += input_ids.size(0)\n",
        "            nb_tr_steps += 1\n",
        "            if FP16:\n",
        "                # modify learning rate with special warm up BERT uses\n",
        "                # if args.fp16 is False, BertAdam is used that handles this automatically\n",
        "                lr_this_step = (\n",
        "                     LR * warmup_linear(global_step/num_train_optimization_steps, WARMUP_PROPORTION))\n",
        "                for param_group in optimizer.param_groups:\n",
        "                    param_group['lr'] = lr_this_step\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            global_step += 1\n",
        "            mb.child.comment = f'loss: {tr_loss:.4f} lr: {optimizer.get_lr()[0]:.2E}'\n",
        "    logger.info(\"  train loss = %.4f\", tr_loss) \n",
        "    return tr_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D_iSu8NTAPnF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
        "all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
        "all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
        "all_targets = torch.tensor([f.target for f in train_features], dtype=torch.float)\n",
        "train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_targets)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BhnC1PqAAPnH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Train only the \"pooler\" and the final linear layer\n",
        "set_trainable(model, True)\n",
        "set_trainable(model.bert.embeddings, False)\n",
        "set_trainable(model.bert.encoder, False)\n",
        "count_model_parameters(model)\n",
        "train(model, num_epochs = 2, learning_rate = 5e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iUJE6c87APnJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Save a trained model\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
        "output_model_file = \"./regressor_stage1.pth\"\n",
        "# torch.save(model_to_save.state_dict(), output_model_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GWDcY8iRAPnK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(output_model_file))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T1R4aAABAPnN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "deL7ZFQmAPnQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Train the last two layer, too\n",
        "set_trainable(model.bert.encoder.layer[11], True)\n",
        "set_trainable(model.bert.encoder.layer[10], True)\n",
        "count_model_parameters(model)\n",
        "train(model, num_epochs = 2, learning_rate = 5e-5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mU0LY7-LAPnT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Save a trained model\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
        "output_model_file = \"./regressor_stage2.pth\"\n",
        "# torch.save(model_to_save.state_dict(), output_model_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8gEKcmojAPnX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Train all layers\n",
        "set_trainable(model, True)\n",
        "count_model_parameters(model)\n",
        "train(model, num_epochs = 1, learning_rate = 1e-5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CFwBuNiEAPna",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Save a trained model\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
        "output_model_file = \"./regressor_stage3.pth\"\n",
        "# torch.save(model_to_save.state_dict(), output_model_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H2U5McjZAPnc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Save a trained model\n",
        "# model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
        "# output_model_file = \"./pytorch_model.bin\"\n",
        "# torch.save(model_to_save.state_dict(), output_model_file)\n",
        "\n",
        "# Load a trained model that you have fine-tuned\n",
        "# model_state_dict = torch.load(output_model_file)\n",
        "# model = BertForSequenceClassification.from_pretrained(args.bert_model, state_dict=model_state_dict, num_labels=num_labels)\n",
        "# model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tcw4nNlrAPng",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "del train_features\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wjer8eqSAPng",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Evauluation"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "Fwa2atbBAPnh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "eval_examples = DoubanRatingProcessor().get_dev_examples()\n",
        "eval_features = convert_examples_to_features(\n",
        "    eval_examples, MAX_SEQ_LENGTH, tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DcL0OjEsAPnj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "logger.info(\"***** Running evaluation *****\")\n",
        "logger.info(\"  Num examples = %d\", len(eval_examples))\n",
        "logger.info(\"  Batch size = %d\", BATCH_SIZE * 5)\n",
        "all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
        "all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
        "all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
        "all_targets = torch.tensor([f.target for f in eval_features], dtype=torch.float)\n",
        "eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_targets)\n",
        "# Run prediction for full data\n",
        "eval_sampler = SequentialSampler(eval_data)\n",
        "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=BATCH_SIZE * 5)\n",
        "\n",
        "model.eval()\n",
        "eval_loss, eval_accuracy = 0, 0\n",
        "nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "mb = progress_bar(eval_dataloader)\n",
        "for input_ids, input_mask, segment_ids, targets in mb:\n",
        "    input_ids = input_ids.to(device)\n",
        "    input_mask = input_mask.to(device)\n",
        "    segment_ids = segment_ids.to(device)\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        tmp_eval_loss = model(input_ids, segment_ids, input_mask, targets)\n",
        "        # outputs = model(input_ids, segment_ids, input_mask)\n",
        "\n",
        "    # outputs = outputs.detach().cpu().numpy()\n",
        "    # targets = targets.to('cpu').numpy()\n",
        "    # tmp_eval_accuracy = accuracy(logits, label_ids)\n",
        "\n",
        "    eval_loss += tmp_eval_loss.mean().item()\n",
        "    # eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "    nb_eval_examples += input_ids.size(0)\n",
        "    nb_eval_steps += 1\n",
        "    mb.comment = f'{eval_loss / nb_eval_steps:.4f}'\n",
        "\n",
        "eval_loss / nb_eval_steps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QcYCnmO4APnn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}